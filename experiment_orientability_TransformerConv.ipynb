{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:47:32.798261Z",
     "start_time": "2024-04-25T17:47:19.035622Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader, ImbalancedSampler\n",
    "from torch_geometric.transforms import FaceToEdge, OneHotDegree\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from mantra.simplicial import SimplicialDataset\n",
    "from mantra.transforms import (\n",
    "    TriangulationToFaceTransform,\n",
    "    OrientableToClassTransform,\n",
    "    DegreeTransform,\n",
    ")\n",
    "from validation.validate_homology import validate_betti_numbers\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: SimplicialDataset(712):\n",
      "====================\n",
      "Number of graphs: 712\n",
      "Number of features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ernst\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch_geometric\\data\\storage.py:450: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'torsion_coefficients', 'betti_numbers', 'n_vertices', 'dimension', 'name', 'orientable', 'genus', 'face'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "\n",
      "Data(dimension=[1], n_vertices=[1], torsion_coefficients=[3], betti_numbers=[3], orientable=[1], genus=[1], name='S^2', face=[3, 4], edge_index=[2, 12], x=[4, 9], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 4\n",
      "Number of edges: 12\n",
      "Average node degree: 3.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "=============================================================\n",
      "Number of orientable Manifolds: 193\n",
      "Number of non-orientable Manifolds: 519\n",
      "Percentage: 0.27, 0.73\n"
     ]
    }
   ],
   "source": [
    "tr = transforms.Compose(\n",
    "    [\n",
    "        TriangulationToFaceTransform(),\n",
    "        FaceToEdge(remove_faces=False),\n",
    "        DegreeTransform(),\n",
    "        OrientableToClassTransform(),\n",
    "        OneHotDegree(max_degree=8, cat=False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SimplicialDataset(root=\"./data\", transform=tr)\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"Dataset: {dataset}:\")\n",
    "print(\"====================\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print(\"=============================================================\")\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f\"Number of nodes: {len(data.x)}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Average node degree: {data.num_edges / len(data.x):.2f}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")\n",
    "\n",
    "print(\"=============================================================\")\n",
    "print(f\"Number of orientable Manifolds: {sum(dataset.orientable)}\")\n",
    "print(\n",
    "    f\"Number of non-orientable Manifolds: {len(dataset) - sum(dataset.orientable)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage: {sum(dataset.orientable) / len(dataset):.2f}, {(len(dataset) - sum(dataset.orientable)) / len(dataset):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 562\n",
      "Number of test graphs: 150\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:-150]\n",
    "test_dataset = dataset[-150:]\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
    "print(f\"Number of test graphs: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=10\n",
    ")  # ,sampler=ImbalancedSampler(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4272,  0.1329],\n",
      "        [ 0.2431,  0.0324],\n",
      "        [ 0.2296,  0.1512],\n",
      "        [ 0.0760, -0.0063],\n",
      "        [ 0.4862,  0.0779],\n",
      "        [ 0.3347,  0.1233],\n",
      "        [ 0.1409,  0.1253],\n",
      "        [ 0.2714,  0.2711],\n",
      "        [ 0.0957,  0.2088],\n",
      "        [ 0.2210,  0.0088]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from operator import concat\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TAGConv, TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = TransformerConv(\n",
    "            dataset.num_node_features, hidden_channels\n",
    "        )\n",
    "        self.conv2 = TransformerConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = TransformerConv(\n",
    "            hidden_channels, hidden_channels, concat=False\n",
    "        )\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(batch.x, batch.edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, batch.edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, batch.edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch.batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.7278, Test Acc: 0.7333\n",
      "Epoch: 002, Train Acc: 0.7509, Test Acc: 0.7600\n",
      "Epoch: 003, Train Acc: 0.8060, Test Acc: 0.8133\n",
      "Epoch: 004, Train Acc: 0.8256, Test Acc: 0.8267\n",
      "Epoch: 005, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 006, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 007, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 008, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 009, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 010, Train Acc: 0.8310, Test Acc: 0.8267\n",
      "Epoch: 011, Train Acc: 0.8310, Test Acc: 0.8267\n",
      "Epoch: 012, Train Acc: 0.8310, Test Acc: 0.8333\n",
      "Epoch: 013, Train Acc: 0.8310, Test Acc: 0.8200\n",
      "Epoch: 014, Train Acc: 0.8381, Test Acc: 0.8200\n",
      "Epoch: 015, Train Acc: 0.8452, Test Acc: 0.8133\n",
      "Epoch: 016, Train Acc: 0.8488, Test Acc: 0.8400\n",
      "Epoch: 017, Train Acc: 0.8505, Test Acc: 0.8400\n",
      "Epoch: 018, Train Acc: 0.8488, Test Acc: 0.8400\n",
      "Epoch: 019, Train Acc: 0.8470, Test Acc: 0.8400\n",
      "Epoch: 020, Train Acc: 0.8470, Test Acc: 0.8400\n",
      "Epoch: 021, Train Acc: 0.8523, Test Acc: 0.8467\n",
      "Epoch: 022, Train Acc: 0.8559, Test Acc: 0.8533\n",
      "Epoch: 023, Train Acc: 0.8559, Test Acc: 0.8400\n",
      "Epoch: 024, Train Acc: 0.8577, Test Acc: 0.8467\n",
      "Epoch: 025, Train Acc: 0.8665, Test Acc: 0.8533\n",
      "Epoch: 026, Train Acc: 0.8737, Test Acc: 0.8467\n",
      "Epoch: 027, Train Acc: 0.8701, Test Acc: 0.8533\n",
      "Epoch: 028, Train Acc: 0.8701, Test Acc: 0.8600\n",
      "Epoch: 029, Train Acc: 0.8737, Test Acc: 0.8533\n",
      "Epoch: 030, Train Acc: 0.8719, Test Acc: 0.8533\n",
      "Epoch: 031, Train Acc: 0.8737, Test Acc: 0.8533\n",
      "Epoch: 032, Train Acc: 0.8665, Test Acc: 0.8467\n",
      "Epoch: 033, Train Acc: 0.8772, Test Acc: 0.8533\n",
      "Epoch: 034, Train Acc: 0.8754, Test Acc: 0.8400\n",
      "Epoch: 035, Train Acc: 0.8772, Test Acc: 0.8600\n",
      "Epoch: 036, Train Acc: 0.8737, Test Acc: 0.8467\n",
      "Epoch: 037, Train Acc: 0.8665, Test Acc: 0.8533\n",
      "Epoch: 038, Train Acc: 0.8790, Test Acc: 0.8400\n",
      "Epoch: 039, Train Acc: 0.8754, Test Acc: 0.8533\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int(\n",
    "            (pred == data.y).sum()\n",
    "        )  # Check against ground-truth labels.\n",
    "    return correct / len(\n",
    "        loader.dataset\n",
    "    )  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 40):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
